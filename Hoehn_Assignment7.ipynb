{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "add6fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark V:  3.3.2\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark Session\n",
    "# Spark Session Name: Assingment07\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get Imports Needed\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "# Get Datatypes needed for DataFrame manipulation\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# Setup Spark Session\n",
    "sc = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Assignment07\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print Spark Version being run\n",
    "print(\"Spark V: \", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0d8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Sentiment Value (0/1) from the line and return as an Integer\n",
    "def GetSentiment(line):\n",
    "    res = line.strip()[-1]\n",
    "    return int(res)\n",
    "get_sentiment = udf(lambda q : GetSentiment(q), IntegerType())\n",
    "\n",
    "# Get the Sentence by removing the last charater and then cleaning it up\n",
    "# Returing the sentence as lower case string\n",
    "def GetSentence(line):\n",
    "    res = line.strip()[:-1].strip()\n",
    "    return res.lower()\n",
    "get_sentence = udf(lambda q : GetSentence(q), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59231e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- sentence: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the three (3) dataset files into dataframes\n",
    "df1 = sc.read.text(\"datasets/amazon_cells_labelled.txt\")\n",
    "df2 = sc.read.text(\"datasets/imdb_labelled.txt\")\n",
    "df3 = sc.read.text(\"datasets/yelp_labelled.txt\")\n",
    "\n",
    "# Concatenate the dataframe df1, df2, and df3 with eachoather to form a \"df\"\n",
    "df = df1.union(df2).union(df3)\n",
    "\n",
    "# Cleanup Data and set two columns (sentence & sentiment) int eh dataframe\n",
    "df = df.withColumn(\"label\", get_sentiment(col(\"value\")))\n",
    "df = df.withColumn(\"sentence\", get_sentence(col(\"value\")))\n",
    "\n",
    "# remove the inital \"value\" column from the dataframe\n",
    "df = df.drop(\"value\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facb7cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Dataframe: 'df'\n",
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|    0|so there is no wa...|\n",
      "|    1|good case, excell...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "This is Dataframe: 'dfWords'\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|so there is no wa...|[so, there, is, n...|\n",
      "|    1|good case, excell...|[good, case,, exc...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "This is Dataframe: 'dfRawFeatures'\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|so there is no wa...|[so, there, is, n...|(7354,[0,2,3,4,5,...|\n",
      "|    1|good case, excell...|[good, case,, exc...|(7354,[23,105,797...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "This is Dataframe: 'dfFeatures'\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|so there is no wa...|[so, there, is, n...|(7354,[0,2,3,4,5,...|(7354,[0,2,3,4,5,...|\n",
      "|    1|good case, excell...|[good, case,, exc...|(7354,[23,105,797...|(7354,[23,105,797...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup MLlib Features\n",
    "from pyspark.ml.feature import IDF, StopWordsRemover, Tokenizer, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Setting up Transformations\n",
    "# Use the Tokenizer to conver the sentence into a word ARRAY / LIST\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# The Count Vectorizer converts the words from the Tokenizer to Vectors\n",
    "countVectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "# IDF is the Inverse Document Frequency - It essentaill sets the imporatnce of the word\n",
    "# IDF is an Estimator which is fit on a dataset and produces an IDF Model\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Setting up the Pipeline for Transformation\n",
    "# This is useful so I can re-use it on my own example data later on in the example\n",
    "pipeline = Pipeline(stages=[tokenizer, countVectorizer, idf])\n",
    "\n",
    "# Let the Pipeline learn from the main datasets previoulsy created\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The below is just an example of what happens to the dataframe -> This will not be used later on\n",
    "# This just helps understand how the columns get added to teh dataframe\n",
    "print(\"This is Dataframe: 'df'\")\n",
    "df.show(2)\n",
    "\n",
    "dfWords = tokenizer.transform(df)\n",
    "print(\"This is Dataframe: 'dfWords'\")\n",
    "dfWords.show(2)\n",
    "\n",
    "model = countVectorizer.fit(dfWords)\n",
    "dfRawFeatures = model.transform(dfWords)\n",
    "print(\"This is Dataframe: 'dfRawFeatures'\")\n",
    "dfRawFeatures.show(2)\n",
    "\n",
    "model = idf.fit(dfRawFeatures)\n",
    "dfFeatures = model.transform(dfRawFeatures)\n",
    "print(\"This is Dataframe: 'dfFeatures'\")\n",
    "dfFeatures.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4745f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split into Training: 2372 and Test: 628\n"
     ]
    }
   ],
   "source": [
    "# Split Dataframe for Trainign and Testing\n",
    "# I opted for a 80 / 20 Split\n",
    "# Using the Random Split I get two arrays each with their respective number of items\n",
    "seed = 77\n",
    "dfTrain, dfTest = df.randomSplit([0.8, 0.2], seed)\n",
    "\n",
    "print(\"Data Split into Training:\", dfTrain.count(), \"and Test:\", dfTest.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23d6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   63|\n",
      "|    0|       0.0|  248|\n",
      "|    1|       1.0|  248|\n",
      "|    0|       1.0|   69|\n",
      "+-----+----------+-----+\n",
      "\n",
      "Estimated Prediction: 0.7898810187955816\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# Using LR for Learing and Best fit\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Building the Parameter for the Cross Validator\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, np.linspace(0.01, 0.3, 10)) \\\n",
    "    .addGrid(lr.elasticNetParam, np.linspace(0.3, 0.8, 6)) \\\n",
    "    .build()\n",
    "\n",
    "# Build and Learning Setup for Cross Validation\n",
    "crossval_lr = CrossValidator(\n",
    "                  estimator = lr,\n",
    "                  estimatorParamMaps = paramGrid_lr,\n",
    "                  evaluator = BinaryClassificationEvaluator()) \n",
    "\n",
    "# Let the LR learn from the Training Dataframe\n",
    "# Note: I initally pass the dataframe to our Pipeline Model for Transformation\n",
    "cvModel_lr = crossval_lr.fit(pipelineModel.transform(dfTrain))\n",
    "\n",
    "# Transform the Dataframe on Our LR Learning Function\n",
    "predictions_lr = cvModel_lr.transform(pipelineModel.transform(dfTest))\n",
    "\n",
    "# Show the Results of the Prediction vs. Labels (supplied in the intial data)\n",
    "predictions_lr.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "# Use the Binary Classigicaiton Evaluator (Simple since we only have 1 / 0) to \n",
    "# show and compare \"labels\" to the \"predictions\"\n",
    "my_eval_lr = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print(\"Estimated Prediction:\", my_eval_lr.evaluate(predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521d45cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|    2|\n",
      "|    1|       1.0|    2|\n",
      "|    1|       0.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n",
      "+-------------------------------------------+-----+----------+\n",
      "|sentence                                   |label|prediction|\n",
      "+-------------------------------------------+-----+----------+\n",
      "|Oh this is really bad i hate coming to work|0    |0.0       |\n",
      "|I love this class!                         |1    |1.0       |\n",
      "|I would like to do more of this stuff      |1    |0.0       |\n",
      "|Waste of my time                           |0    |0.0       |\n",
      "|Going to school is great                   |1    |1.0       |\n",
      "+-------------------------------------------+-----+----------+\n",
      "\n",
      "Estimated Prediction: 0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Testing a totally New Dataset created by Richard Hoehn as test to see how well the model works\n",
    "# This was just for fun to see how it might work using the LR Model later on when gettting more\n",
    "# data to test with.\n",
    "dfRichard = sc.createDataFrame(\n",
    "    [\n",
    "        (0, \"Oh this is really bad i hate coming to work\"),\n",
    "        (1, \"I love this class!\"),\n",
    "        (1, \"I would like to do more of this stuff\"),\n",
    "        (0, \"Waste of my time\"),\n",
    "        (1, \"Going to school is great\")\n",
    "    ], [\n",
    "        \"label\", \"sentence\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We will use Richard's made up example to see how well it did\n",
    "predictions_lr = cvModel_lr.transform(pipelineModel.transform(dfRichard))\n",
    "predictions_lr.groupBy('label','prediction').count().show()\n",
    "predictions_lr.select(\"sentence\", \"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "my_eval_lr = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "print(\"Estimated Prediction:\", my_eval_lr.evaluate(predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1a9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
